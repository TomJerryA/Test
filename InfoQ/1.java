<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>Google Is Now Indexing JavaScript and CSS Content: Pitfalls to Avoid</h3><p>Google has announced that Googlebot, its web crawler, now executes and indexes some content in JavaScript. This change could impact negatively search results, unless a few basic rules are taken into account.</p>
<p>In a <a href="http://googlewebmastercentral.blogspot.com.es/2014/05/understanding-web-pages-better.html">post on Google's Webmaster Central Blog</a>, developers Erik Hendriks and Michael Xu, and Webmaster Trends Analyst Kazushi Nagayama, advise web developers that some changes that have taken place in the way Google handles JavaScript content when indexing web pages could impact negatively search results and provide a few hints as to how to prevent that.</p>
<p>According to Hendriks, Xu, and Nagayama, &quot;in the past few months, Google's indexing system has been rendering a substantial number of web pages more like an average user’s browser&quot; would when JavaScript has been turned on. This differs from the way Google traditionally indexed pages, i.e., by &quot;only looking at the raw textual content that we’d get in the HTTP response body and didn't really interpret what a typical browser running JavaScript would see.&quot;</p>
<p>Sometimes, they write, JavaScript rendering will not have the expected outcome, &quot;which may negatively impact search results for a site&quot;. To prevent this from occurring, they are offering some hints at potential problems and possible ways around them:</p>
<ul> 
 <li>Sites blocking JavaScript or CSS files from access will not allow Google's indexing system to see a site like an average user. The recommendation is to allow for JavaScript and CSS files access in robots.txt. This is especially relevant for mobile sites, since JavaScript and CSS files would allow Google algorithms to understand that a page is optimized for mobile.</li> 
 <li>Web servers should also be able to handle the volume of crawl requests for resources, otherwise rendering results may be affected.</li> 
 <li>JavaScript code which is too complex or arcane could also prevent rendering the page fully and accurately.</li> 
 <li>Sometimes, JavaScript is used to remove content from a page rather than adding; this will make the removed content inaccessible to Google indexing engine.</li> 
</ul>
<p>Finally, suggest Hendriks, Xu and Nagayama, it's always a good idea to have a webpage degrade gracefully. This will make its content accessible to search engines that can't execute JavaScript yet.</p>
<p>Google is working on a tool that should be available soon to help webmasters better understand how Googlebot renders JavaScript and CSS content.</p><br><br><br><br><br><br></body></html>