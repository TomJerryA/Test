<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>Hadoop虚拟研讨会</h3><p>现今，大数据和Hadoop在计算机工业里正如暴风骤雨般开展着。从CEO、CIO到开发人员，每个人对其用法都有自己的看法。据Wikipedia所述：</p><blockquote> 
 <p>“Apache Hadoop是一个<a href="http://en.wikipedia.org/wiki/Open_source">开源</a>的<a href="http://en.wikipedia.org/wiki/Software_framework">软件框架</a>，它支持数据密集型的<a href="http://en.wikipedia.org/wiki/Distributed_computing">分布式应用</a>，许可授权隶属于Apache v2 license.<a href="http://en.wikipedia.org/wiki/Apache_Hadoop#cite_note-0">[1]</a> 它使应用程序以<a href="http://en.wikipedia.org/wiki/Petabytes">拍字节</a>（petabytes） 级数据进行工作，并可以在成千上万台独立的计算机上运行。Hadoop源自于<a href="http://en.wikipedia.org/wiki/Google">Google</a>的<a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a> 和 <a href="http://en.wikipedia.org/wiki/Google_File_System">Google File System </a>(GFS) 两篇论文。现在通常认为完整的Apache Hadoop‘平台’由Hadoop内核、<a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a> 和<a href="http://en.wikipedia.org/wiki/Hadoop_Distributed_File_System">HDFS</a>组成，以及若干相关的项目——包括<a href="http://en.wikipedia.org/wiki/Apache_Hive">Apache Hive</a> 、Apache <a href="http://en.wikipedia.org/wiki/Hbase">Hbase</a>等等”</p> 
</blockquote><p>可惜这个定义并没有真正解释Hadoop及其在企业中的角色。</p><p>在本次虚拟座谈会中，InfoQ采访了多位Hadoop提供商和用户，他们就Hadoop的现在和将来发表了看法，并讨论了Hadoop继续走向成功并进一步推广的关键。</p><p><strong>参加者</strong>：</p><ul> 
 <li>Omer Trajman， Cloudera技术解决方案副总裁</li> 
 <li>Jim Walker，Hortonworks产品总监</li> 
 <li>Ted Dunning，MapR首席应用架构师</li> 
 <li>Michael Segel，芝加哥 Hadoop 用户群创始人</li> 
</ul><p><strong>问题</strong>：</p><ol> 
 <li>你如何定义Hadoop？作为架构师，我们对服务器、数据库等术语有更专业的思考。在你的心里Hadoop属于哪个层面？</li> 
 <li>尽管人们实际谈论的是Apache Hadoop，但他们却很少直接从Apache网站上下载。如今大多数人都使用Cloudera、Hortonworks、MapR、Amazon等等的“发行版”进行安装。你认为这一现象的原因是什么，这些“发行版”有哪些不同（请供应商们客观一点，我们知道你的是最好的）。</li> 
 <li>你认为如今Hadoop最普遍的用法是什么？将来呢？</li> 
 <li>除了Flume、Scribe 和 Scoop之外，Hadoop与其他企业计算的集成非常少。你认为在企业IT基础架构中，Hadoop会开始扮演更大的角色吗？</li> 
 <li>除了著名的<a href="http://static.usenix.org/event/osdi10/tech/full_papers/Peng.pdf">Percolator</a>之外，现在大部分Google项目都是用Hadoop实现的。你认为这样的项目(应该)在Apache的跟踪范围内吗？你了解实时Hadoop的其他方向吗？</li> 
 <li>许多人设法提高使用Hadoop的技能。还有很多人去找熟悉Hadoop的人。但是仍然搞不清楚，如何掌握Hadoop技术？阅读<a href="http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/0596521979">这本书</a>？参加培训？还是考认证？</li> 
</ol><p><strong>问题1：你如何定义Hadoop？作为架构师，我们对服务器、数据库等术语有更专业的思考。在你的心里Hadoop属于哪个层面？</strong></p><p><strong>Omer Trajman</strong>： Hadoop是一个新型数据管理系统，它通过计算网络的处理能力将传统的非结构化领域或非关系型数据库联合起来。虽然它从传统大规模并行处理（MPP）数据库设计模式借鉴了大量经验，但Hadoop有几个关键的不同。首先，它是为低成本字节的经济而设计的。Hadoop几乎可以在任意硬件上运行，可以非常宽容地应对异构配置和不时发生的故障。第二，Hadoop非常容易扩展。Hadoop第一个版本就可以扩展到数千个节点，当前版本试验表明可以持续增加到上万个节点以上。使用主流的两插槽8核处理器，那就是80，000核的计算能力。第三，Hadoop可以非常灵活地存储和处理数据的类型。Hadoop可以接受任何格式、任何类型的数据，并具有一组功能丰富的API，用来读取和写入任何格式的数据。</p><p><strong>Jim Walker</strong>： Hadoop是一个高度可扩展的开源数据管理软件，使您轻松地获取、处理、交换任何数据。Hadoop几乎可以连接到传统企业数据栈的每一层，因此将占据数据中心的中心位置。它将在系统和用户之间交换数据并提供数据服务。在技术层面，Hadoop也就是做这些事情，但因为它为大众带来了超级计算能力，它也造成了商业的转变。它是开源软件，所创建的社区带来大规模并行处理，以及水平扩展所有在商品硬件上的存储。它不能替代系统，它迫使现有工具更加专业化，并占有流行的数据架构工具箱的一席之地。</p><p><strong>Ted Dunning</strong>： 非常精确地定义Hadoop，至少让每个人都同意你的意见恐怕是不可能的。即使如此，假设你考虑这两个定义，你可以得到非常接近的答案：</p><p>a. 同名的Apache项目，该项目已经发布了一个map-reduce的实现和一个分布式文件系统。</p><p>b. 一组项目集合，Apache和一些其他项目，它们使用或者以某种方式关联到Apache Hadoop项目。</p><p>第一个定义也经常用来暗指由Apache Hadoop项目发布的软件，但是，一个软件到底要与发布版本多接近才能（或应该）称之为Hadoop或Hadoop衍生品，是一个颇有争议的话题。</p><p>对于我来说，作为社区中Hadoop相关软件的主要使用者或开发者，我更喜欢一个不太常用的Hadoop的定义。对于术语“Hadoop”，我更加喜欢先用它来代表社区，其次是主要的代码或项目。对我来说，社区比任何单个项目代码更重要。</p><p><strong>Michael Segel</strong>： 我把Hadoop看作是一个框架以及进行分布式或并行处理的一组工具。你在HDFS中的分布式存储，在Job Tracker和Task Trackers中的分布式计算模型，以及在HBase中的分布式持久对象存储。按照Hadoop的定位，我认为这取决于特定的解决方案。</p><p>很难将Hadoop归为单独的类别。在一些场景中Hadoop用来做中间处理，这是难以在传统的RDBMS中完成的事情，于是使用Hadoop作为中间步骤，最后使用他们已有的BI工具执行分析。还有人用Hadoop来提供 “实时”（主观认为的）数据处理。集成Lucene / SOLR和已有的Hadoop / HBase作为实时搜索引擎的一部分。关键是Hadoop被用于解决不同组织不同类型的问题；即使在同一个企业中。这可能Hadoop最大的一个优势，它是一个基础框架，可以用来解决不同类型的问题。更多的人在使用Hadoop，并将其推进到极限；这将产生大量各种各样的解决方案。</p><p><strong>问题2：尽管人们实际谈论的是Apache Hadoop，但他们却很少直接从Apache网站上下载。如今大多数人都使用Cloudera、Hortonworks、MapR、Amazon等等的“发行版”进行安装。你认为这一现象的原因是什么，这些“发行版”有哪些不同（请供应商们客观一点，我们知道你的是最好的）。</strong></p><p><strong>Omer Trajman</strong>：“发行版”的需求有两个主要来源。至于Cloudera，四年前我们开始与客户交流时首次遇到一个要求。每一位客户运行在不同的基础版本上，并且打了不同的小补丁，还使用了不同版本的客户端库。一旦我们涉及多个客户，就无法提供支持和解决代码问题的架构原则。产生“发行版”的第二个理由是，合作伙伴需要能够测试客户正在运行的代码库，以发现它们的不利影响。如果一个合作伙伴在CDH4认证，一个客户在CDH4上运行，他们要确信他们的软件在工作中不会产生冲突。为了给每一个人(不仅仅是我们的客户)创建一个标准基线，Cloudera在2009年首次创建了CDH。如果客户运行在CDH 4.2上，每个人都确切地了解代码库中有什么，并且可以测试它的不利影响。因为它完全开源并经Apache许可，任何人都可以按需改变安装程序，并且，他们可以获得任何人的支持。</p><p><strong>Jim Walker</strong>： 我们认为基于Hadoop平台的下载趋势是一个较新的现象。因为越来越多的组织意识到，他们将受益于Hadoop，当他们首次开始使用这项技术时，他们在寻找易于使用和消费的体验。后来，他们着手感兴趣的工具，工具易于管理和监控Hadoop在生产环境中的运行。这些“发行版”有很多相同点，相比开源的Apache Hadoop，它们更易于使用和运维。这些“发行版”之间最主要的区别是，大多数（而非所有）包括了专属的软件组件。这么做的确把用户锁定在一个特定的“发行版”上，不允许他们充分利用开放源码社区过程。但是，Hadoop及其相关项目都有自己的发布周期和版本结构，每个“发行版”都为消费者提供了重要价值，“发行版”已经把Hadoop和所有已知相关的项目打包到一起，负责部署的人就不需要去单独获取各个Hadoop相关的项目、单独测试并自行维护这样一个复杂的解决方案的网络了。</p><p><strong>Ted Dunning</strong>： 事实上，人们去下载已打包的发行版无非是出于节省时间和节省脑力的考虑——否则，他们需要一个一个去下载必要的组件，一个一个地安装，并想办法让它们保持兼容。通常人们关注个别项目的功能或Hadoop社区项目，然后从Apache上直接下载项目源码，然而，还有一种做法是使用标准化的“发行版”。这使他们可以集中精力去处理那些他们认为最重要的事。</p><p>实际上，搭建和测试完整的Hadoop“发行版”是一个最重要的任务。使用标准的“发行版”大大简化这个过程。另外，所有重要的“发行版”各自还有一些专属的Hadoop附件，使某些事可以更好地工作。Hortonworks有基于VMWare的软件，他们试图使namenode更加强壮，Cloudera有专属管理接口，MapR有专属文件系统、表存储和管理功能。用户可以很轻易地找到这些附件的价值。</p><p><strong>Michael Segel</strong>：首先，我想要指出的是，还是有很多人从Apache 网站直接下载Hadoop的。那些人可能是想要研究最新版本的可用性，可能是Apache新的访问者。讨论组上的人仍在询问兼容性的问题，像“我有X版本的HBase，我需要哪个版本的Hadoop……” 他们使用某个供应商的“发行版”，就不会有这个问题了。</p><p>企业倾向于选择供应商，因为他们需要供应商的支持。他们希望降低在Hadoop集群上的投入，以专注于解决手边的问题。供应商提供免费版本和支持版本，还有工具去简化他们产品的下载。(他们也有自己的网站，并有很多有用的帮助信息)。</p><p>当讨论供应商时，我必须提前声明，所有供应商实际上都是Apache Hadoop的衍生品。虽然Cloudera和Hortonworks承诺100%是Apache的，但我仍然认为它们是衍生品。供应商必须决定应用哪些补丁，亦或回退他们的版本。虽然所有代码都100%是Apache的，但这还是造成了一些轻微的差异。另外，供应商的产品中还包括一些附加工具和专属软件。我并没有说衍生品不好，相反它是好东西。衍生品也有利于区分供应商之间的差异。</p><p>我恰巧在供应商间保持着中立。你提到所有的供应商都支持Apache Hadoop API，所以你编写的map/reduce程序，最多只做一次编译就可以在每一个版本中运行。</p><p>正如我之前所说，Hortonworks和Cloudera承诺100%是Apache的，但采用了是Hadoop提供的内核。Cloudera长期占有大部分的市场份额。Hortonworks借他们新出的1.0发行版发起强劲的营销攻势。我认为随着时间的推移，将难以在这两个之间做出选择。归根结底，它将成为支持和辅助工具。Hadoop还在不断地发展。像诸如YARN、HCatalog、Impala和Drill，在此我只列出了一些最近的创新。供应商不能自满，必须确定他们将来打算进一步支持的功能。</p><p>MapR稍有不同。自全面启动以来，MapR已决定替换内核，把HDFS替换为自己的MapRFS，同时MapRFS实现了所有的HDFS API，它是一个兼容POSIX的文件系统，可以通过NFS安装。此外，他们解决了在Apache的内核产品中发现的SPOF Name Node问题。他们还在一年前的初始版本中提供HA(高可用性)。虽然MapR支持Apache Hadoop API，但他们的软件是自有的、闭源的。MapR有三种版本，M3（免费版）、M5（含有支持的版本，并启用了所有HA特性）和M7（在最近和自己重写的HBase一起发布），MapR采取了一种与其他供应商不同的方法，它肯定会有属于自己的追随者。</p><p>亚马逊拥有他们自己的产品，在他们提供的EMR中包括了对MapR的M3和M5的支持。因为他们不卖产品，所以我不会把他们归类到供应商。而且我认为你还要把Google加进来，因为他们在最近发布了他们的竞争产品。在此我们看到又一个MapR的合作伙伴进入到Hadoop市场。</p><p>在所有考查的选项和各种各样的供应商之中，我不得不说消费者将是最终的胜利者。早在90年代的时候，因为Informix、Oracle和Sybase间的激烈竞争，我们目睹了RDBMS的演变。今天？我认为市场还相当不够成熟，我们在进行着一段疯狂之族。</p><p><strong>问题3：你认为如今Hadoop最普遍的用法是什么？将来呢？</strong></p><p><strong>Omer Trajman</strong>： 如今，人们用Hadoop应对各种行业间各自不同的挑战。最为普遍的用法是用来加速ETL。例如金融服务，不再是针对每个事务从众多源系统中拉数据，而是由源系统将数据推至HDFS，ETL引擎处理数据，然后保存结果。ETL流程可被写入Pig 或 Hive中，或者使用商业解决方案如Informatica、 Pentaho、 Pervasive等等。结果可以将来用Hadoop分析，也可以提交到传统报表和分析工具来分析。经证实，使用Hadoop存储和处理结构化数据可以减少10倍的成本，并可以提升4倍处理速度。比传统ETL更突出的是，Hadoop还可以用来收集内部系统（比如应用和web日志）以及远程系统(在网络和全球上)的遥测数据。把精细的感应数据提供给公司的能力模型（如通讯和移动载流能力模型），预测网络和设备上可能发生的问题，并主动地采取措施。Hadoop还可以作为集中式数据集线器，执行从跨组织的数据集分析到预测分析平台的任何工作。这些应用如今广泛地部署在生产环境中，为收集所有组织数据提供了可能性，很好地驱动了业务的发展。</p><p><strong>Jim Walker</strong>：大部分组织刚刚开始他们的Hadoop旅程。他们用它来提炼大量的数据，为业务分析实践提供价值。有些人用它来采集和使用一些曾经废弃的数据，或者仅仅从已有系统中去采集比以前更多的数据。更先进的组织开始走向数据科学研究之路，从事大数据和传统输入源的探索。在2013年，Hadoop将成为主流，人们将慎重地考虑将其作为传统企业数据架构的一部分，同ETL、RDBMS、EDW和目前所有那些为组织提供数据的现有工具一样成为“一等公民”。</p><p><strong>Ted Dunning</strong>： 我不了解当前的主要用途，但我看到了两个无限增长的领域，以至于我打算减少大部分大数据其他领域的使用。它们是：</p><p>a. 世界度量系统。这些系统包括各种产品，它们拥有巨大的度量能力，并极有可能产生大量的数据。例如，涡轮机供应商使用仪器检测喷气式发动机，所以每架飞机将成为巨大的数据来源，又如磁盘驱动器供应商正要在单独的磁盘驱动器中建设家用电话系统。同样地，零售商通过文字去察看客户对店内商品的反应。所有这些应用程序有可能比现有的大部分大数据系统产生更多的数据。</p><p>b. 基因组织系统。单人的人类基因组测序就可以产生约四分之一TB的数据。癌细胞的增长包括细胞群落，经常有成千上万的突变遍布数以百计(至少)的变体发育谱系。这些不同的细胞系表示它们的基因不同，这些也是可以度量的。这意味着在不久的将来，一个人的医疗记录很有可能增长到几个TB的规模。再乘以每年进行医疗护理的人数呢，这表明当前的电子医疗系统的规模可能小了四到六个数量级。</p><p>还有一些我们暂时还不知道系统，它们可能会产生更多的数据。</p><p><strong>Michael Segel</strong>： 这是一个很难回答的问题。我认为我们会看到更多的公司从Hive的实施开始，因为它是“挂得最低的水果”。许多公司的员工都了解SQL，对他们来说，Hive的学习曲线最短。所以公司可以用很短的时间实现价值，能够很容易地部署Hive去解决问题。随着更多的公司内部Hadoop能力的提升，我想他们将会利用其他的Hadoop组件。</p><p>因为Hadoop是一个相当通用的框架，它可以用于许多不同行业中各种各样的解决方案。例如，通过采集和处理传感器数据以确定在你商店的货架上摆放哪些品牌的洗衣粉。你也可能有不同的用法。它是用来作为解决问题的中间步骤，还是用来提供实时数据？因为可以通过一些额外的工具来扩展Hadoop，我认为，我们可以期待看到这些工具被公司更多的应用，这些公司的需求很难使用已有工具集来实现。在Hadoop HDFS框架之上，有HBase(包含在Hadoop发行版)和Accumulo（由美国DoD社区创建，是‘Big Table’的另一个衍生品）。而在HBase之上，你有OpenTSDB和Wibidata去扩展HBase的功能。随着这些工具的成熟和增强，我认为它们将会得到更多的应用。</p><p>我认为，随着更多的公司采用Hadoop并开始理解它的潜力，我们将继续看到Hadoop用于不同的方面，解决更为复杂的问题。</p><p><strong>问题4：除了Flume、Scribe 和 Scoop之外，Hadoop与其他企业计算的集成非常少。你认为在企业IT基础架构中，Hadoop会开始扮演更大的角色吗？</strong></p><p><strong>Omer Trajman</strong>： Hadoop正在迅速地成为IT模型的数据中心。由于Flume中有用于任何事件数据丰富的连接，Sqoop可用于所有结构化数据，HttpFS可用于SOA集成和ODBC、JDBC报表工具，所以任何现有数据管理工作流程在产生数据或请求数据时都可以无缝地、安全地与Hadoop接口通讯。这种扩展使Hive进一步发展为Hadoop元仓库的标准。最初的目的是在非结构化数据上映射SQL模式，Hive已经增强了安全功能（针对定义验证和数据访问控制），而且集成了Flume、Sqoop以及其他系统接口。如今所有Hadoop部署都用这些数据集成的功能与Hadoop交换数据。未来，他们将能够与Hive安全地交换元数据。</p><p><strong>Jim Walker</strong>： 你忘了，在Hadoop堆栈还有一个重要的组件。该组件被称为Apache HCatalog，它为Hadoop提供了一个集中式元数据服务，这样就能够更容易地与传统的系统传递和交换数据了。HCatalog分解了结构化数据与Hadoop之间的阻抗不匹配（impedance mismatch），这样它们就可以使二者深度集成。Teradata的大量分析设备和微软的HDInsights产品都是很好的例子。在企业数据架构中，Hadoop最终在现有系统旁扮演了属于它自己的角色。</p><p><strong>Ted Dunning</strong>： 我自己的MapR公司，有一个业务是销售定制的Hadoop“发行版”，它比其他“发行版”更容易集成企业基础设施系统。客户发现它们构建计算系统的能力真的很强，它们可以交叉扩展并与现有应用程序一起工作。在大部分大数据应用中，只有部分情况是真正的大规模计算。一个大数据系统的许多组件实际上并非那么大。因此，对于大数据系统的小部件，它可以通过使用小数据技术来获得回报，并节省出时间去开发系统中的大数据部件。</p><p><strong>Michael Segel</strong>： 我认为这有点误导。Flume、Scribe和Scoop都是开源的(Apache)项目，旨在整合Hadoop与其他公司的基础设施。IBM(数据阶段)和Informatica在他们的产品中添加了Hadoop集成。此外，Quest Software也同样创建了一些解决方案以帮助与Hadoop的集成。所以业界也出现了在企业产品中适应并采用Hadoop的尝试。事实上如果我们与所有主要的硬件和软件公司进行交流，我们会发现他们都有一些Hadoop解决方案，作为他们产品的一部分。</p><p>总体而言，我们仍然处于Hadoop增长曲线的早期。随着更多的公司采用Hadoop作为基础设施的一部分，我们将看到来自传统供应商更多的工具。许多现有的BI工具供应商利用Hive Thrift服务器连接他们的应用。在不足的方面，数据可视化算是其中的一个领域，我认为我们可以期待看到更多的工具进入市场。许多大型企业寄希望于BI报表和dashboard工具。与其购买一个完全独立的工具，也许不如为了支持Hadoop去扩展现有的基础设施更加让人关注。</p><p><strong>问题5：除了著名的</strong>Percolator<strong>之外，现在大部分Google项目都是用Hadoop实现的。你认为这样的项目(应该)在Apache的跟踪范围内吗？你了解实时Hadoop的其他方向吗？</strong></p><p><strong>Omer Trajman</strong>： 谷歌有许多项目，它们都是特定于谷歌的需求而创建的。除Percolator之外，还有许多项目、一些已公开的以及其他仍受严密保护的秘密。并非所有项目都是有用的或者可适用于其他的组织。从历史观点上说，社区和赞助组织指望从谷歌和其他大数据公司寻找灵感，以解决如何处理非常巨大的数据管理问题。BigTable 启发了 HBase、Chubby启发了Zookeeper ，F1启发了Impala 的一些命名。如今，Hadoop已经提供了若干解决方案，用于实时数据抽取(Flume)、实时数据存储(HBase)和实时数据查询(Impala)。未来的发展很可能被Hadoop用户和开发社区直接驱动。</p><p><strong>Jim Walker</strong>： 我们认为，这里有Hadoop的应用案例，如果需要较快的数据访问，那么就适合用Hadoop来收集和分析。当然有很多其他不需要快速交互式访问数据的应用案例。我们相信充分利用社区驱动过程，整个Hadoop市场会最好的服务于广大全开放式项目。这个过程已经被证实，它可以满足企业的预期，为企业创造稳定和可靠的软件。Apache软件基金会保证了这一点。</p><p><strong>Ted Dunning</strong>： 其实我觉得Hadoop社区只是实现了很小一部分谷歌的项目。在应用程序级别，暂时没有F1、Spanner、Dremel和Sawzall等替代品，Borg在开源社区中也没有好的替代品，谷歌内部使用基于开关的Open Flow、操作系统虚拟层或各种各样庞大的基础设施服务。还有些类似的项目，他们一般都是一些逊色的仿制品。例如，我以为Mahout无法抗衡Google的机器学习系统，Lucene要想比得上谷歌的搜索系统也有很长的路要走，甚至Hadoop比不上谷歌内部同一阵营的map-reduce系统。</p><p>开源社区终于有了适合于谷歌源代码管理系统的Github形式，但是，如果我们想为全世界提供一些功能，那么我们在开源社区中还要做更多大量的工作。</p><p><strong>Michael Segel</strong>： 我想，如果我们要看近期在扩展和加强Hadoop上的某些讨论，我认为你可以参考一下Percolator。在Quora上，我们可以看到一些活跃的专题<a href="http://www.quora.com/Big-Data/What-are-the-opensource-alternatives-to-Googles-Percolator-Dremel-and-Pregel">讨论</a>。</p><p>当我们在HBase中有协处理器时，这里有一个相对较新的项目——Drill，其中Ted Dunning也积极参与在内。(我会尊重Ted针对Drill的讨论)。最近Cloudera也公布了Impala。(我会尊重Cloudera针对Impala的讨论。)</p><p>有一些公司如HStreaming，也参与了入站数据的实时处理。HStreaming是一个本地公司，作为我们的芝加哥Hadoop用户组(CHUG)代表出席了会议。(他们已经搬到了加利福尼亚。)</p><p>实时处理方面，我认为协处理器的使用，以及Lucene和Solr更紧密的集成，我们将看到更多HBase/Hadoop的应用案例。</p><p><em>*问题6：许多人设法提高Hadoop的应用技能。还有很多人去找熟悉Hadoop的人。但是仍然搞不清楚，如何掌握Hadoop技术？阅读<a href="http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/0596521979">这本书</a>？参加培训？考认证？ *</em></p><p><strong>Omer Trajman</strong>： 获取Hadoop技能最好的方式，是参与一个Hadoop项目。刚开始做Hadoop项目时最好的方式是参加培训班，参加认证考试并保持在项目中有一两本可以随时取用的好书。找一些专门研究Hadoop和更大生态系统的公司，跟他们讨论也非常有用。虽然大多数项目以Hadoop为核心开始启动，但很快就扩展到数据抽取处理、数据服务、分析和工作流。我们推荐与一个组织合作，它们可以提供一个从培训到服务完整的Hadoop平台， 跨所有堆栈和整个Hadoop的生命周期支持和管理软件。虽然Hadoop是一个具有挑战性的技术，但它如今已经为组织解决了一些最大的问题，根据我们的经验，这很值得投资。</p><p><strong>Jim Walker</strong>： 培训是快速学习Hadoop基础的最好方式。一旦掌握了这些，就可以做些Hadoop实践并开始学习更多的概念了。</p><p><strong>Ted Dunning</strong>： 我们最大的一些客户开始喜欢在令人意想不到的地方寻找人才。而不是在通常几所顶级的大学中争夺那几个水平相仿的聪明人，他们可以在二流学校中找到绝对聪明的学生，尤其 (大致而言) 是在非计算机科学领域的技术人员。然后对他们培训大数据方面的技术，在我曾经见过的人中，这些人都做得很好。</p><p>我还遇到相当多的人去参加HUG（Hadoop用户组）或其他兴趣小组见面会，他们努力的从事自我训练。这些人利用所有可能的方式来学习技能，包括在线课程(Andrew Ng机器学习方面的课程很受欢迎)、参加会议、创建个人项目、努力在工作中寻找大数据分析需求。</p><p>我感觉，这两个趋势给我们的启示是：很难把更多的聪明人放在公司里，而且在大数据方面，幕后更为广泛的称职人员的工作效果可能比灵光一闪式的想法更好。这个世界仍在快速发展，全能型人才的技能在开拓性事业中非常具有价值。但是，专家最终可能会占主导地位，但也要再等几年之后。与此期间，会涌现出各种优秀的人才。</p><p><strong>Michael Segel</strong>： 我认为你漏掉了最重要的……动手实践的经验。虽然读书、在线指南和示例很重要。培训有助于加速我们学习的过程，但是在Hadoop方面，动手实践的经验是不可替代的。</p><p>有几个成本较低的方式可以获得这种经验。首先，大部分供应商的产品都有一个可以下载的免费版本。它可以在一台机器上在虚拟分布模式下运行Hadoop。虽然它们大多数都需要Linux，但有一个可以在微软上运行的产品。我们不要忘了亚马逊。它可以加速和减速单独的机器和集群，以运行Map/Reduce任务和测试你的代码。</p><p>另外，像Infochimps这类公司将数据集放在S3上供公共使用和下载。这就使那些没有硬件和基础设施的人可以非常容易地去运行Hadoop。</p><p>此外，世界各地还有许多Hadoop相关的用户组。所以可以很容易找到一个离你很近的用户组，如果还没有，那么你也可以创建一个。找到同样有志于学习Hadoop的人，能使经历更加有趣，而且过程也不会那么痛苦。</p><p>最后同样重要的是，你可以到讨论组和论坛中提问，你会得到答案的。</p><p><strong>英文原文链接</strong>：<a href="http://www.infoq.com/articles/HadoopVirtualPanel;jsessionid=92A43B948F4E869ACBEA7EEFFF1290DF">Hadoop Virtual Panel</a></p><hr /><p>感谢<a href="http://www.infoq.com/cn/bycategory.action;jsessionid=92A43B948F4E869ACBEA7EEFFF1290DF?authorName=马国耀">马国耀</a>对本文的审校。</p><p>给InfoQ中文站投稿或者参与内容翻译工作，请邮件至<a href="mailto:editors@cn.infoq.com">editors@cn.infoq.com</a>。也欢迎大家通过新浪微博（<a href="http://www.weibo.com/infoqchina">@InfoQ</a>）或者腾讯微博（<a href="http://t.qq.com/infoqchina">@InfoQ</a>）关注我们，并与我们的编辑和其他读者朋友交流。</p><div class="clearer-space"></div></body></html>