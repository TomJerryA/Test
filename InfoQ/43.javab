<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>Orchestrating Your Delivery Pipelines with Jenkins</h3><p>In a <a href="http://www.infoq.com/articles/preparing-for-cd-in-the-enterprise">previous article</a>, we covered useful preparation steps for your Continuous Delivery implementation, including defining your pipeline phases, preconditions and required approvals, owners and access control requirements, resource requirements such as number of concurrent build machines, identifying which phases can be run in parallel for speedup, and more.</p>
<p>Here, we will discuss how to put a number of these recommendations into practice in a concrete setting, namely setting up a delivery pipeline in <a href="http://jenkins-ci.org/">Jenkins</a>. Many of the steps we will present carry over to other Continuous Integration (CI) and orchestration tools, and there are analogous extensions or core features for many of the plugins we will introduce, too.</p>
<p>We are focussing here on Jenkins, however, because it is the most widely-used Continuous Integration server out there. Even if you are using different CI servers or services in your environment, it should be relatively easy to experiment with the steps we will cover in a “sandbox” Jenkins installation, before carrying them over to your own CI environment.</p>
<h2>Prerequisites</h2>
<p>Before diving into building our delivery pipeline with Jenkins, we need to discuss two important prerequisites: that our pipeline, or at least the part of the pipeline that we are looking to implement here (going all the way to Production may not be the most sensible initial goal), is</p>
<ul> 
 <li>Predictable and standardized, i.e. that the steps and phases we want to run each time the pipeline is triggered are the same</li> 
 <li>Largely automated. We will cover ways to handle manual approvals to “bless” a certain build, but that is about it.</li> 
</ul>
<p>If the current release process does not display these characteristics, i.e. every release ends up being a little bit different, or there are still many manual steps (reviewing test plans, preparing target environments) that are required, building a pipeline via a Continuous Integration tool or generic automation orchestrator may not be the most appropriate step at this point.</p>
<p>It is probably advisable to first increase the level of standardization and automation, and to look at tools such as <a href="http://xebialabs.com/products/xl-release">XL Release</a> in the “release coordination” or “Continuous Delivery release management” category to help with that.</p>
<div id="lowerFullwidthVCR"></div>
<h2>What We Will Cover</h2>
<p>We will cover the following topics to help build your delivery pipeline:</p>
<ol> 
 <li><a href="#_Toc1">Ensuring reproducible builds</a></li> 
 <li><a href="#_Toc2">Sharing build artifacts throughout the pipeline</a></li> 
 <li><a href="#_Toc3">Choosing the right granularity for each job</a></li> 
 <li><a href="#_Toc4">Parallelizing and joining jobs</a></li> 
 <li><a href="#_Toc5">Gates and approvals</a></li> 
 <li><a href="#_Toc6">Visualizing the pipeline</a></li> 
 <li><a href="#_Toc7">Organizing and securing jobs</a></li> 
</ol>
<h4>Our example project</h4>
<p>In order to make some of the scenarios and approaches we will outline more tangible, we’ll base this discussion around a sample development project. Let’s assume we’re working on the server-side component of a mobile app for Android and iOS. Our delivery process for our application is as follows:</p>
<ol> 
 <li>Whenever a code change is committed, we build the code and, if successful, package up the current version as a candidate version for release (<i>Basic Build &amp; Package</i> job).</li> 
 <li>Now that we know that the code compiles and passes our unit tests, we trigger a code quality build that carries out a bunch of static analysis to verify code quality (<i>Static Code Quality Analysis</i> job).</li> 
 <li>The static analysis can take quite some time, so in parallel we deploy the candidate version to two functional testing environments, one for the Android and one for the iOS app version, in preparation for testing (jobs <i>Deploy to Android Func Test Env </i>and <i>Deploy to iOS Func Test Env</i>). We use two test environments so we can easily identify differences in how the backend behaves when talking to either version of the app.</li> 
 <li>When both deployments have completed, we trigger functional tests, with the iOS and Android apps talking to their respective backend (<i>Func Tests</i> job).</li> 
 <li>If the functional tests pass, we carry out parallel deployments of our release candidate to a regression and a performance test environment (jobs <i>Deploy to Regr Test Env</i> and <i>Deploy to Perf Test Env</i>). The completion of each deployment triggers the appropriate tests (jobs <i>Regr Test</i> and <i>Perf Test</i>).</li> 
 <li>If the regression and performance tests and our static code analysis build from earlier complete successfully, the candidate is made available for business approval, and the business owner is notified.</li> 
 <li>The business owner can approve, in a manual step, the candidate build.</li> 
 <li>Approval triggers and automated deployment to production (<i>Deploy to Prod</i> job).</li> 
</ol>
<p>Schematically, our delivery pipeline looks like this:</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig1-large.png" _href="resource://Fig1-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/4Fig1.png" alt="" _href="img://4Fig1.png" _p="true" /></a></p>
<p><b><small>Figure 1: Our example project’s delivery pipeline</small></b></p>
<p>It’s worth noting that this is not intended to be interpreted as a “good”, “bad” or “recommended” pipeline structure. The pipeline that works best for you will not be a direct copy of this example, but will depend on <i>your</i> applications and <i>your</i> process!</p>
<h2><a name="_Toc1">Ensuring reproducible builds</a></h2>
<p>One of the key principles of our pipeline is that we produce <i>one</i> set of build artifacts which we will pass through the various pipeline stages for testing, verification and, ultimately, release. We want to be sure that this is a reliable process and that this initial build is carried out in a reproducible way that does not somehow depend on the local dependency cache of the slave we happen to be building on, for example. In our project, we’ve taken the following steps to help achieve this:</p>
<h4>Using clean repositories local to the workspace</h4>
<p>We’ve configured the build system to use a clean repository local to the build job’s <i>workspace</i>, rather than one that is shared by all builds on that slave. This will ensure the build does not happen to succeed because of an old dependency that is no longer available from your standard repositories, but that happened to be published to that slave’s repo at some point. Consider clearing your build job’s workspace regularly (most <a href="https://wiki.jenkins-ci.org/display/JENKINS/Plugins#Plugins-Sourcecodemanagement">SCM plugins</a> have a “clean build” option, and for things like partial cleanup the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Workspace+Cleanup+Plugin">Workspace Cleanup plugin</a> can help), or at least wiping its local repo. For Maven builds, the build repository location can easily be configured via the main Jenkins settings, and overridden per job where necessary.</p>
<h4>Using clean slaves based on a known template</h4>
<p>We can take this a step further by running our builds on “clean” slaves created on demand and initialized to a known, reproducible state where possible. Plugins such as the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Amazon+EC2+Plugin">Amazon EC2 plugin</a>, <a href="https://wiki.jenkins-ci.org/display/JENKINS/Docker+Plugin">Docker plugin</a> or <a href="https://wiki.jenkins-ci.org/display/JENKINS/JClouds+Plugin">jclouds plugin</a> can be used for this purpose, and there are also hosted services such as CloudBees&nbsp;<a href="http://www.cloudbees.com/dev">DEV@cloud</a> that provide this functionality. Spinning up build slaves on demand also has the substantial advantage of helping avoid long build queue times if you only have a limited pool of slaves and a growing number of pipeline runs.</p>
<h4>Using a central, shared repository for build dependencies</h4>
<p>We’re using a centralized artifact repository across all our projects, rather than allowing each project to decide from where to download build dependencies. This ensures that two projects that reference the same dependency will actually get the <i>same</i> binary, and allows us to enforce dependency policies (such as banning certain dependencies) in a central location. If you are using a build system that supports Maven-based dependency management, a Maven proxy such as <a href="http://www.sonatype.org/nexus/">Nexus</a> or <a href="http://www.jfrog.com/home/v_artifactory_opensource_overview">Artifactory</a> is ideal.</p>
<h2><a name="_Toc2">Sharing build artifacts throughout the pipeline</a></h2>
<p>Once we have built our candidate artifact in our initial build job, we need to find a way to ensure exactly <i>this</i> artifact is used by all the subsequent builds in our pipeline.</p>
<h4>Retrieving build artifacts from upstream jobs</h4>
<p>Jenkins provides a couple of ways to share artifacts produced by an upstream job with subsequent downstream jobs. We are using the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Copy+Artifact+Plugin">Copy Artifact plugin</a>, which allows us to retrieve build artifacts from another job with a convenient build step. We’re copying from a <i>fixed</i> build (i.e. specified by build number or build parameter), which is preferable to referring to a <i>variable</i> upstream build (such as the “Last successful build” option). In the latter case, we cannot be sure that we will be referencing the artifacts that triggered <i>this</i> pipeline run, rather than those produced by a subsequent commit.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig2-large.png" _href="resource://Fig2-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/1Fig2.png" alt="" _href="img://1Fig2.png" _p="true" /></a></p>
<p><b><small>Figure 2: Copying pipeline artifacts using the Copy Artifact plugin</small></b></p>
<h4><i>Alternatives</i></h4>
<ul> 
 <li>If you also want to access the artifact outside Jenkins, you can save the candidate artifact as a “Build Artifact” of the initial job, then use the Jenkins APIs to download it (e.g. using wget or cURL) in downstream jobs</li> 
 <li>If you want to treat candidate artifacts as build dependencies, the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Jenkins+Maven+Repository+Server">Jenkins Maven Repository Server plugin</a> makes build artifacts available via a Maven repo-compliant interface, which can be used by Maven, Gradle, Ant and other build tools to retrieve artifacts. It also provides additional options for referencing artifacts via the SHA1 ID of the Git commit that produced the artifacts (especially useful if the Git commit ID is your unique build identifier), as well as for accessing artifacts of a chain of linked builds.</li> 
 <li>If you already maintain a Definitive Software Library <i>outside</i> Jenkins, you can create a setup similar to that offered by the Maven Repo Server plugin with an external Maven repo. In that case, you would publish the artifacts to the repo using a Maven identifier that includes the build number, commit ID or whatever is considered a stable identifier or unique ID.</li> 
</ul>
<h4>Identifying the correct upstream build throughout the pipeline</h4>
<p>Whichever alternative we choose, we need to pass a stable identifier to downstream builds so we can pick the right candidate artifact for our pipeline run. In our pipeline, we have made most of the downstream builds parameterized and are using the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Parameterized+Trigger+Plugin">Parameterized Trigger plugin</a> to pass the identifier.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig3-large.png" _href="resource://Fig3-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/3Fig3.png" alt="" _href="img://3Fig3.png" _p="true" /></a></p>
<p><b><small>Figure 3: Passing the unique pipeline identifier to downstream builds</small></b></p>
<h4><i>Alternatives</i></h4>
<ul> 
 <li>We can also use the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Delivery+Pipeline+Plugin">Delivery Pipeline plugin</a> (which we will also meet later), which optionally creates an environment variable that is available in all downstream jobs.</li> 
</ul>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig4-large.png" _href="resource://Fig4-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/3Fig4.png" alt="" _href="img://3Fig4.png" _p="true" /></a></p>
<p><b><small>Figure 4: The pipeline version environment variable option of the Delivery Pipeline plugin</small></b></p>
<h4>Using Fingerprints to track artifact usage</h4>
<p>However you end up passing the stable pipeline identifier to downstream pipeline phases, setting up all jobs in the pipeline to use <a href="https://wiki.jenkins-ci.org/display/JENKINS/Fingerprint">Fingerprints</a> is almost always a good idea. Jenkins “fingerprints” artifacts by storing their MD5 checksums, and using these to track use of an artifact across jobs. It allows us to check, at the end of a pipeline run, which artifacts have been used in which builds, and so to verify that our pipeline has indeed consistently been testing and releasing the correct artifact. Jenkins provides a post-build task that allows us to explicitly record fingerprints for files in the workspace. Certain plugins, such as the Copy Artifact plugin, automatically fingerprint artifacts when copying them from an upstream build, in which case we can omit the post-build step</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig5-large.png" _href="resource://Fig5-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/2Fig5.png" alt="" _href="img://2Fig5.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig6-large.png" _href="resource://Fig6-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/1Fig6.png" alt="" _href="img://1Fig6.png" _p="true" /></a></p>
<p><b><small>Figure 5: Fingerprinting build artifacts using the Copy Artifact plugin and via a post-build action</small></b></p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig7-large.png" _href="resource://Fig7-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig7-small.png" hspace="100" alt="" _href="img://Fig7-small.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig8-large.png" _href="resource://Fig8-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/1Fig8.png" _href="img://1Fig8.png" _p="true" hspace="100" alt="" /></a></p>
<p><b><small>Figure 6: Tracking the usage of build artifacts via Fingerprints</small></b></p>
<h2><a name="_Toc3">Choosing the right granularity for each job</a></h2>
<p>This may seem a rather obvious point, but choosing the correct granularity for each job, i.e. how to distribute all the steps in our pipeline across multiple jobs, will help us make our pipeline more efficient and allow us to identify bottlenecks more easily. As a rough rule of thumb, every stage in your pipeline can be represented by a separate job or, in the case of multi-dimensional tests, a matrix job. This is why, for instance, we have not combined build and deployment to the test environments, or deployment to the regression test environment and the regression tests themselves, into a single job in our pipeline. If, for instance, we merged <i>Deploy to Regr Test </i>and <i>Regr Test</i> into one “multi-stage” job and it has failed 10 times recently, we would need to spend time analysing the failures to figure out if the deployment or the tests themselves are the real problem.</p>
<p>The flipside of avoiding “multi-stage” jobs is, of course, that there are more jobs that we need to manage and visualize: 10 already, in our relatively simple example.</p>
<h2><a name="_Toc4">Parallelizing and joining jobs</a></h2>
<p>Especially when running multi-platform tests, but also if we are building artifacts for different target platforms, we want to make our pipeline as efficient as possible by running builds in parallel. In our case, we want to parallelize our functional tests for Android and iOS, as well as running the performance and regression tests in parallel. We’re using a couple of Jenkins mechanisms for this:</p>
<h4>Running parallel instances of the same job with different parameters</h4>
<p>For the functional tests, which are “variants” of the same build (same steps, but different configuration parameters), we’re using the standard <a href="https://wiki.jenkins-ci.org/display/JENKINS/Building+a+matrix+project">Multi-Configuration project</a> type (often referred to as a “matrix build”). If we needed to handle potentially spurious failures for some of the matrix builds, we could also add the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Matrix+Reloaded+Plugin">Matrix Reloaded plugin</a>.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig9-large.png" _href="resource://Fig9-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig9.png" alt="" _href="img://Fig9.png" _p="true" /></a></p>
<p><b><small>Figure 7: <i>Func Tests</i> in our sample pipeline is a multi-configuration (“matrix”) project</small></b></p>
<h4>Running different jobs in parallel</h4>
<p>For the deployments to the two functional test environments, where we need to run different jobs, we’re using the standard option is simply to use build triggers to kick off multiple downstream jobs in parallel once the upstream job (<i>Basic Build and Package</i>, in our case) completes</p>
<h4><i>Alternatives</i></h4>
<ul> 
 <li>If you want to coordinate sets of parallel jobs, you might also consider the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Multijob+Plugin">Multijob plugin</a>, which adds a new project type that allows multiple jobs to run in parallel (in fact, it could potentially be used to orchestrate multiple pipeline phases).</li> 
</ul>
<h4>Joining parallel sections of the build pipeline</h4>
<p>For our matrix job <i>Func Tests</i> build, “joining”, i.e. waiting until all the parallel builds have been completed before continuing to the downstream phases <i>Deploy to Regr Test Env </i>and <i>Deploy to Perf Test Env</i>, is handled automatically by the matrix job type. We have configured <i>Func Tests</i> to trigger the downstream builds on success, and these will only be triggered if both the Android and iOS builds in the matrix complete successfully.</p>
<p>For the deployment to the two functional test environments, where we simply trigger multiple jobs to run in parallel, we end up facing the “diamond problem”: how to join the parallel jobs <i>Deploy to Android Func TestEnv</i> and <i>Deploy to iOS Func Test Env </i>back together to trigger <i>one</i> subsequent job, <i>Func Tests</i>. Here, we’re using the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Join+Plugin">Join plugin</a>, which we’ve configured in the job “at the top” of the diamond to trigger the job “at the bottom” of the diamond once the parallel deployment jobs have completed successfully. We do not need specify the deployment jobs explicitly – the plugin kicks off the <i>Func Tests</i> job once all direct downstream jobs have finished. The Join plugin also supports passing of build parameters, which we need to identify the build artifacts for this pipeline run.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig10-large.png" _href="resource://Fig10-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/1Fig10.png" alt="" _href="img://1Fig10.png" _p="true" /></a></p>
<p><b><small>Figure 8: Triggering <i>Func Tests</i> in our sample pipeline by using the Join plugin to wait for the direct downstream jobs <i>Deploy to Android Func Test Env</i> <u>and</u> <i>Deploy to iOS Func Test Env</i> to complete</small></b></p>
<h4>Handling more complex job graphs</h4>
<p>If you have more complicated “job graphs”, you may also want to have a look at the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Flow+Plugin">Build Flow plugin</a>, which allows you to define job graphs, including parallel sections and joins, programmatically.</p>
<h2><a name="_Toc5">Gates and approvals</a></h2>
<p>As the pipeline stages get closer to the QA and Production environments, many organizations require some form of sign-off or approval before tasks can be carried out. In our case, we require a manual sign-off from the business owner before kicking off the <i>Deploy to Prod</i> job, for instance.</p>
<p>As previously noted, Jenkins and other CI tools and generic orchestrators do not offer very comprehensive support for manual pipeline tasks, but there <i>are</i> a couple of options to handle approvals.</p>
<h4>Supporting approvals based on multiple conditions</h4>
<p>We’re using the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Promoted+Builds+Plugin">Promoted Builds plugin</a>, which offers manual approval (and a corresponding email notification to the approver) as one of a number of possible ways to “promote” a build. It also supports a variety of “on promotion” actions, including triggering downstream jobs.</p>
<p><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/3Fig11.png" alt="" _href="img://3Fig11.png" _p="true" /></p>
<p><b><small>Figure 9: The <i>Basic Build and Package</i><i> </i>job triggers a production deployment after manual approval by the business owner and confirmation that all downstream jobs have successfully completed</small></b></p>
<h4><i>Alternatives</i></h4>
<ul> 
 <li>A simple option to create a very basic gate can be just to ensure that the “gated” downstream job is only triggered manually and can only be executed by a limited number of approvers. In this case, the simple fact of triggering a build constitutes approval. This pattern can also be automated, using e.g. the <a href="https://wiki.jenkins-ci.org/display/JENKINS/ScriptTrigger+Plugin">ScriptTrigger plugin</a> to search for an approval in an external system.<br /> However, this breaks the approach of using parameterized triggers to pass on required information, such as the unique artifact ID. If we adopt this pattern, we need to find another way to ensure the appropriate information is passed, e.g. by prompting the approver to enter the appropriate parameters manually, or by having the trigger script retrieve them from the approval record (e.g. a JIRA ticket).</li> 
 <li>If you only want to ensure that a task is manually triggered but do not need to track multiple conditions, you might want to look at the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Build+Pipeline+Plugin">Build Pipeline plugin</a>, which provides a post-build step to manually execute downstream projects.</li> 
</ul>
<p>This step also allows parameters, such as our build identifier, to be passed to the manually triggered downstream job.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig12-large.png" _href="resource://Fig12-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig12.png" alt="" _href="img://Fig12.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig13-large.png" _href="resource://Fig13-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig13.png" alt="" _href="img://Fig13.png" _p="true" /></a></p>
<p><b><small>Figure 10: The Build Pipeline plugin’s post-build step and manual trigger in the pipeline view</small></b></p>
<h2><a name="_Toc6">Visualizing the pipeline</a></h2>
<p>Being able to provide a clear, highly accessible visualization of our build pipelines is very important for a successful Continuous Delivery implementation. Not just to ensure the team is always aware of the current pipeline state, but also as a mechanism to communicate to the business and other stakeholders.</p>
<h4>Using standard views</h4>
<p>Views are standard Jenkins features we are using to collect the jobs that constitute our pipeline in one overview. The Multijob plugin, which we briefly mentioned above, provides a similar “list-style” view. A drawback of both alternatives, however, is that these views show the <i>currently executing</i> builds for each job in the pipeline, which may be working on <i>different</i> release candidates. For example, the <i>Perf Tests</i> and <i>Regr Tests</i> jobs may be testing one particular candidate version while the <i>Basic</i> <i>Build and Package</i> job is already busy with the next commit.</p>
<p><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig14.png" alt="" _href="img://Fig14.png" _p="true" /></p>
<p><b><small>Figure 11: A standard list view showing active jobs working on <i>different</i> release candidates</small></b></p>
<h4>Specialized Delivery Pipeline views</h4>
<p>From a Continuous Delivery perspective, however, we want to see all the builds that make up a particular <i>instance</i> of a pipeline run, i.e. all the builds related to one candidate application version. Two dedicated plugins that support this kind of view are the Build Pipeline plugin and the Delivery Pipeline plugin. Note that both plugins fail to capture the link to the <i>Deploy to Prod</i> job, which is not an immediate downstream build, but triggered by the Promoted Builds plugin.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig15-large.png" _href="resource://Fig15-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig15.png" alt="" _href="img://Fig15.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig16-large.png" _href="resource://Fig16-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig16.png" alt="" _href="img://Fig16.png" _p="true" /></a></p>
<p><b><small>Figure 12: Build Pipeline and Delivery Pipeline plugin views of our sample pipeline</small></b></p>
<h2><a name="_Toc7">Organizing and securing jobs</a></h2>
<h4>Handling many jobs</h4>
<p>Even if each of our pipelines only consists of a handful of jobs, once we start setting up pipelines for multiple projects or versions, we’ll soon have many Jenkins jobs to manage. In our case, with 10 phases per pipeline, we’d quickly be looking at 100 or more jobs to manage! Creating one or multiple views per pipeline is an obvious approach, but it still leaves us with an incredibly large “All jobs” view in Jenkins – not fun to navigate and manage (in fact, it starts to get so big that you may want to consider <a href="https://wiki.jenkins-ci.org/display/JENKINS/Editing+or+Replacing+the+All+View">replacing it entirely</a>). It generally also requires us to adopt job naming conventions along the lines of <i>myProject-myVersion-pipelinePhase</i>, so that all jobs for a pipeline are listed together, and so that we can use regular expressions when defining views, rather than having to select pipeline jobs for a view individually.</p>
<h4>Configuring access control</h4>
<p>Where this approach starts to create challenges is when we start to implement access control policies for our pipelines. We need to ensure that different phases of the pipeline have <i>different</i> access control policies (in our example, developers are not authorized to trigger the QA jobs or the deployment to production), and setting these policies on each job individually is very maintenance-intensive and error-prone.</p>
<p>In our example, we’re using the <a href="https://wiki.jenkins-ci.org/display/JENKINS/CloudBees+Folders+Plugin">CloudBees Folders plugin</a> in combination with the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Matrix+Authorization+Strategy+Plugin">Matrix Authorization Strategy plugin</a>. The combination allows both for convenient job organization <i>and</i> efficient access control configuration. We’ve organized our pipeline jobs in three folders, <i>MyProject/1 – Developer Jobs</i>, <i>My Project/2 – QA Jobs</i> and <i>MyProject/3 – Business Owner Jobs</i>, with each pipeline job in the appropriate folder. Folders are compatible with standard list views, so we can keep our existing <i>MyProject Jobs</i> view. Importantly, we can define access control policies at the folder level, which is <i>much</i> more convenient than having to secure individual jobs.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig17-large.png" _href="resource://Fig17-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig17.png" alt="" _href="img://Fig17.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig18-large.png" _href="resource://Fig18-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig18.png" alt="" _href="img://Fig18.png" _p="true" /></a></p>
<p><b><small>Figure 13: The CloudBees Folders plugin in action, with folder-level security configured using the Matrix Authorization Strategy plugin</small></b></p>
<h4><i>Alternatives</i></h4>
<ul> 
 <li>If you want to apply permissions based on the job name, an option you can consider is the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Role+Strategy+Plugin">Role Strategy plugin</a>, which allows you to define different roles that are scoped to different parts of a pipeline. One drawback is that the jobs to which a role definition applies are determined by a regular expression. This can lead to additional complexity in the job naming scheme though (<i>myProject-myVersion-owningGroup-pipelinePhase</i>, anyone?), and can be brittle if jobs are renamed.</li> 
</ul>
<h4>Good practice: versioning your Jenkins configuration</h4>
<p>One additional thing we’re doing in our example, a good Jenkins practice in pretty much all circumstances, is to version out job configurations. This allows us to easily track any changes and revert to earlier configurations, if necessary. We’re using both the <a href="https://wiki.jenkins-ci.org/display/JENKINS/JobConfigHistory+Plugin">JobConfigHistory plugin</a> (which provides a nice diff view) and <a href="https://wiki.jenkins-ci.org/display/JENKINS/SCM+Sync+configuration+plugin">SCM Sync Configuration plugin</a> (which stores the configuration off-disk in a repository), but depending on your needs typically one or the other will suffice.</p>
<p><b><small>(Click on the image to enlarge it)</small></b></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig19-large.png" _href="resource://Fig19-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig19.png" alt="" _href="img://Fig19.png" _p="true" /></a></p>
<p><a href="/resource/articles/orch-pipelines-jenkins/en/resources/Fig20-large.png" _href="resource://Fig20-large.png"><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/Fig20.png" alt="" _href="img://Fig20.png" _p="true" /></a></p>
<p><b><small>Figure 14: The JobConfigHistory plugin’s diff view and the configuration settings for the SCM Sync Configuration plugin</small></b></p>
<h2>Conclusion</h2>
<p>Setting up Continuous Delivery pipelines in Jenkins that are secure, efficient, and easy to use and manage can quickly become challenging. We’ve discussed important prerequisites, made a number of recommendations and introduced a set of freely available plugins which can make the process a whole lot easier. Hopefully, you are now in a better position to identify whether Jenkins is the right orchestrator for your current process, to build pipelines painlessly and to quickly start making life better for your teams and delivering business value to your customers!</p>
<h2>About the Authors</h2>
<p><strong><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/AndrewPhillips.jpg" vspace="3" hspace="3" align="left" alt="" _href="img://AndrewPhillips.jpg" _p="true" />Andrew Phillips</strong> is VP of Products for XebiaLabs, providers of application delivery automation solutions. Andrew is a cloud, service delivery and automation expert and has been part of the shift to more automated application delivery platforms.&nbsp; In his spare time as a developer, he worked on Multiverse, the open-source STM implementation, contributes to Apache jclouds, the leading cloud library and co-maintains the Scala Puzzlers site.</p>
<p>&nbsp;</p>
<p><strong><img src="http://www.infoq.com/resource/articles/orch-pipelines-jenkins/en/resources/kkawaguchi.jpg" vspace="3" hspace="3" align="left" alt="" _href="img://kkawaguchi.jpg" _p="true" />Kohsuke Kawaguchi</strong> is the creator of Jenkins. He is a well-respected developer and popular speaker at industry and Jenkins community events. He's often asked to speak about his experience and approach in creating Jenkins; a CI platform that has become a widely adopted and successful community-driven open source project. The principles behind the Jenkins community - extensibility, inclusiveness, low barriers to participation - have been the keys to its success. Kawaguchi's sensibilities in creating Jenkins and his deep understanding of how to translate its capabilities into usable software have also had a major impact on CloudBees' strategy as a company. Before joining CloudBees, Kawaguchi was with Sun Microsystems and Oracle, where he worked on a variety of projects and initiated the open source work that led to Jenkins.</p><br><br><br><br><br><br></body></html>