<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>Modern Enterprise Performance Analysis Antipatterns</h3><h2>Defining Performance Analysis</h2>
<p>There are many definitions of “performance analysis”, but in my opinion one of the most useful is:</p>
<blockquote> 
 <p>A measurement-driven approach to understanding how an application behaves under load.</p> 
</blockquote>
<p>The merit of this definition is that it calls attention to measurement as being key to the entire process, and by simple extension, also draws attention to statistics and data analysis as activities likely to be important to the performance engineer.</p>
<p>Going further, it helps to position performance analysis as a fundamentally empirical activity, that resembles an experimental science with inputs and outputs.</p>
<p>These outputs can then be framed as questions with quantitative answers - such as:</p>
<blockquote>
 At 10x customers, will the system have enough memory to cope?
</blockquote>
<blockquote>
 What is average response time customers see from the application?
</blockquote>
<blockquote>
 What does the rest of that distribution look like?
</blockquote>
<blockquote>
 How does that compare to our competitors?
</blockquote>
<p>In this formulation, performance as expressed by these best practices is more science than art; an activity which is fundamentally quantitative and which has a direct relationship to business activities.</p>
<div id="lowerFullwidthVCR"></div>
<p>However, despite these attributes, performance has often languished in a state where even well-known best practices lag behind the reality of practitioners.</p>
<p>There are a number of different models that might explain this, but one interesting possibility is provided by Carey Flichel in the superb piece &quot;Why Developers Keep Making Bad Technology Choices&quot;</p>
<p>In the post, Carey specifically calls out five main reasons that cause developers to make bad choices:</p>
<ul> 
 <li>Boredom</li> 
 <li>Resume (or “CV” if you're British) Padding</li> 
 <li>Peer Pressure</li> 
 <li>Lack of Understanding of Existing System</li> 
 <li>Misunderstood / Non-Existent Problem</li> 
</ul>
<p>In this article, we present some of the most common performance analysis antipatterns in the enterprise platform, and try to express them in terms of their basic causes as enumerated by Carey. The specific examples that led to the distillates below are drawn from the Java ecosystem, but similar remarks apply to many other types of enterprise system.</p>
<p>Each basic cause corresponds to some common cognitive bias. For example, <i>Boredom</i> and <i>Resume Padding</i> both stem from a desire to escape the existing tech that a developer uses in his or her day job, and their aspirational desire for a better tomorrow.</p>
<p>The antipatterns are presented below, in a style and format that should be reminiscent of the Gang of Four, as well, of course, as the antipattern format pioneered by Brown et al.</p>
<h2>AntiPattern Catalogue</h2>
<h3><i>Distracted By Shiny</i></h3>
<p><strong>Description</strong></p>
<p>Newest or coolest tech is often first tuning target</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>It's teething trouble - we need to get to the bottom of it</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>This is just a shot in the dark</li> 
 <li>Developer does not really understand the new tech</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Boredom</li> 
 <li>Resume Padding</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>This antipattern is most often seen with younger teams. Eager to prove themselves, or to avoid becoming tied to what they see as 'legacy' systems, they are often advocates for newer, &quot;hotter&quot; technologies - which may, coincidentally, be exactly the sort of technologies which would confer a salary uptick in any new role.</p>
<p>Therefore, the logical subconscious conclusion to any performance issue is to first take a look at the new tech - after all, it's not properly understood, so a fresh pair of eyes would be helpful, right?</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Measure to determine real location of bottleneck</li> 
 <li>Ensure adequate logging around new component&nbsp;</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/divider3.jpg" /></p>
<h3><i>Distracted By Simple</i></h3>
<p><strong>Description</strong></p>
<p>The simplest parts of the system are targeted first</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>Let's get into this by starting with the parts we understand</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Dev understands how to tune (only?) that part of the system</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Lack of Understanding of Existing System</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>The dual of &quot;Distracted by Shiny&quot;, this antipattern is often seen in an older, more established team, which may be more used to a maintenance/keep-the-lights-on role. If their application has recently been merged or paired with newer technology, the team may feel intimidated or not want to engage with the new systems.</p>
<p>Under these circumstances, developers may feel more comfortable by only profiling those parts of the system that are familiar, hoping that they will be able to achieve the desired goals without going outside of their comfort zone.</p>
<p>Of particular note is that both of these first two antipatterns are driven by a reaction to the unknown; in &quot;Distracted by Shiny&quot; this manifests as a desire by the developer (or team) to learn more and gain advantage - essentially an offensive play. By contrast, &quot;Distracted by Simple&quot; is a defensive reaction - to play to the familiar rather than engage with a potentially threatening new technology.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Measure to determine real location of bottleneck</li> 
 <li>Ask for help from domain experts if problem is in an unfamiliar component</li> 
</ul>
<p>&nbsp;</p>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://1divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/1divider3.jpg" /></p>
<h3><i>UAT is My Desktop</i></h3>
<p><strong>Description</strong></p>
<p>UAT environment differs significantly from PROD</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>A full-size UAT environment would be too expensive</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Outages caused by differences in environments are almost always more expensive than a few more boxes</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Misunderstood / Non-Existent Problem</li> 
</ul>
<p><strong>Discussion</strong></p>
<p><i>UAT is my Desktop</i> stems from a different kind of cognitive bias than we have previously seen. This bias insists that doing some sort of UAT must be better than doing none at all. Unfortunately, this hopefulness fundamentally misunderstands the complex nature of enterprise environments. For any kind of meaningful extrapolation to be possible, the UAT environment must be production like.</p>
<p>In modern adaptive environments, the runtime subsystems will make best use of the available resources. If these differ radically from the target deployment, they will make different decisions under the differing circumstances - rendering our hopeful extrapolation useless at best.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Track the cost of outages and opportunity cost related to lost customers</li> 
 <li>Invest in a UAT environment that is identical to PROD</li> 
 <li>In most cases, the cost of the first, far outweighs the second</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://2divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/2divider3.jpg" /></p>
<h3><i>PROD-like Data is Hard</i></h3>
<p><strong>Description</strong></p>
<p>Data in UAT looks nothing like PROD</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>It's too hard to keep PROD and UAT in synch</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Data in UAT must be PROD-like for accurate results</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Lack of Understanding of Existing System</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>This antipattern also falls into the trap of &quot;something must be better than nothing&quot;. The idea is that testing against even out-of-date and unrepresentative data is better than not testing.</p>
<p>As before, this is an extremely dangerous line of reasoning. Whilst testing against <i>something</i> (even if it is nothing like PROD data) at scale can reveal flaws and omissions in the system testing, it provides a false sense of security.</p>
<p>When the system goes live, and the usage patterns fail to conform to the expected norms that have been anchored by UAT data, the development and ops teams may well find that they have become complacent due to the warm glow that UAT has provided, and are unprepared for the sheer terror that can quickly follow an at-scale production release.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Consult data domain experts and invest in a process to migrate PROD data back into UAT</li> 
 <li>Over-prepare for at-scale go-lives.</li> 
 <li>Wherever possible, have dedicated &quot;worse-case&quot; teams or tools (e.g. <a href="http://www.codinghorror.com/blog/2011/04/working-with-the-chaos-monkey.html" target="_blank">Chaos Monkey</a>)</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://3divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/3divider3.jpg" /></p>
<h3><i>Performance Tips (aka Tuning By Folklore)</i></h3>
<p><strong>Description</strong></p>
<p>Code and parameter changes are being applied blind</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>I found these great tips on Stack Overflow. This changes everything.</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Developer does not understand the context or basis of performance tip and true impact is unknown</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Lack of Understanding of Existing System</li> 
 <li>Peer Pressure</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>A performance tip is a workaround for a known problem - essentially a solution looking for a problem. They have a shelf life and usually date badly - someone will come up with a solution that will render the tip useless (at best) in a later release of the software or platform.</p>
<p>One source of performance advice that is usually particularly bad would be admin manuals. They contain general advice devoid of context - this advice and &quot;recommended configurations&quot; is often insisted on by lawyers, as an additional line of defense if the vendor is sued.</p>
<p>Java performance happens in a specific context, with a large number of contributing factors. If we strip away that context, what is left is almost impossible to reason about, due to the complexity of the execution environment.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Only apply well-tested and well-understood techniques, which directly affect the most important aspects of a system.</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://4divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/4divider3.jpg" /></p>
<h3><i>Blame Donkey</i></h3>
<p><strong>Description</strong></p>
<p>Certain components are always identified as the issue</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>It's always JMS / Hibernate / A_N_OTHER_LIB</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Insufficient analysis has been done to reach this conclusion</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Peer Pressure</li> 
 <li>Misunderstood / Non-Existent Problem</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>This antipattern is often displayed by management or the business, as in many cases they do not have a full understanding of the technical stack and so are proceeding by pattern matching or have unacknowledged cognitive biases. However technologists are also far from immune to this antipattern.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Resist pressure to rush to conclusions</li> 
 <li>Perform analysis as normal</li> 
 <li>Communicate the results of analysis to all stakeholders (in order to try to encourage a more accurate picture of the causes of problems).</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://5divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/5divider3.jpg" /></p>
<h3><i>Fiddle With Switches</i></h3>
<p><strong>Description</strong></p>
<p>Team becomes obsessed with JVM switches</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>If I just change these settings, we’ll get better performance</p> 
</blockquote>
<p><strong>Reality</strong></p>
<ul> 
 <li>Team does not understand impact of changes</li> 
</ul>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Lack of Understanding of Existing System</li> 
 <li>Misunderstood / Non-Existent Problem</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>The JVM has literally hundreds of switches - this provides a highly configurable runtime, but gives rise to a great temptation to make use of all of this configurability. This is usually a mistake - the defaults and self-management capabilities are usually sufficient. Some of the switches also combine with each other in unexpected ways - which makes blind changes even more dangerous.</p>
<p><strong>Resolutions</strong></p>
<p>Before putting any change to switches live:</p>
<ul> 
 <li>Measure in PROD</li> 
 <li>Change 1 switch at a time in UAT</li> 
 <li>Test change in UAT</li> 
 <li>Retest in UAT</li> 
 <li>Have someone recheck your reasoning</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://6divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/6divider3.jpg" /></p>
<h3><i>Microbenchmarking</i></h3>
<p><strong>Description</strong></p>
<p>Tuning effort is focused on some very low-level aspect of the system</p>
<p><strong>Example Comment</strong></p>
<blockquote> 
 <p>If we can just speed up method dispatch time...</p> 
</blockquote>
<p><strong>Reality</strong></p>
<p>Overall system-level impact of micro-changes is utterly unknown</p>
<p><strong>Root causes</strong></p>
<ul> 
 <li>Lack of Understanding of Existing System</li> 
 <li>Misunderstood / Non-Existent Problem</li> 
 <li>Resume Padding</li> 
 <li>Peer Pressure</li> 
</ul>
<p><strong>Discussion</strong></p>
<p>Performance tuning is a statistical activity, which relies on a highly specific context for execution. This implies that larger systems are usually easier to benchmark than smaller ones - because with larger systems, the law of large numbers works in the engineers favor, helping to correct for effects in the platform that distort individual events.</p>
<p>By contrast, the more we try to focus on a single aspect of the system, the harder we have to work to unweave the separate subsystems (e.g. threading, GC, scheduling, JIT compilation, etc.) of the complex environment that make up the platform (at least in the Java / C# case). This is extremely hard to do, and the handling of the statistics is sensitive, and is not often a skillset that software engineers have acquired along the way. This makes it very easy to produce numbers that do not accurately represent the behavior of the system aspect that the engineer believed they were benchmarking.</p>
<p>This has an unfortunate tendency to combine with the human bias to see patterns, even when none exist. Together, these effects lead us to the spectacle of a performance engineer who has been deeply seduced by bad statistics or a poor control - an engineer arguing passionately for a performance benchmark or effect that their peers are simply not able to replicate.</p>
<p><strong>Resolutions</strong></p>
<ul> 
 <li>Do not microbenchmark unless you know you have a known use case for it; and do so publicly, and in the company of your peers.</li> 
 <li>Be prepared to be wrong a lot, and have your thinking challenged repeatedly.</li> 
</ul>
<p style="margin-left: 80px;"><img width="400" _p="true" _href="img://7divider3.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/7divider3.jpg" /></p>
<h2>Conclusion</h2>
<p>Why has performance tuning acquired these antipatterns? What is it about the tuning process that encourages cognitive biases which lead to such incorrect conclusions?</p>
<p>Key to these questions is understanding that software engineering is fundamentally different from other engineering disciplines. In a wide range of mechanical engineering systems, the physical properties of small components are well understood, and the composition effects lead to only small amounts of (often well-studied) emergent behavior.</p>
<p>Software is different. The systems we build are far more elaborate than those typically found elsewhere in human endeavor. This is both because we work with very simple basic parts, and also because we have built tools which enable us to work with very large numbers of basic parts. Unfortunately, (or fascinatingly, depending on your point of view) as software has grown more complex, we have discovered that it has a highly emergent nature. This means that unexpected phenomena have manifested as our complexity has increased - and as we have discussed in this article, not all of them are positive.</p>
<h2>Acknowledgements</h2>
<p>Special thanks to Martijn Verburg, Kirk Pepperdine, Trisha Gee and James Gough (and others) for elucidating (and in several cases, naming) these antipatterns for me.</p>
<h2>About the Author</h2>
<p><strong><img align="left" _p="true" _href="img://Ben-Evans.jpg" alt="" src="http://www.infoq.com/resource/articles/Performance-Analysis-Antipatterns/en/resources/Ben-Evans.jpg" />Ben Evans</strong> is the CEO of jClarity, a Java/JVM performance analysis startup. In his spare time he is one of the leaders of the London Java Community and holds a seat on the Java Community Process Executive Committee. His previous projects include performance testing the Google IPO, financial trading systems, writing award-winning websites for some of the biggest films of the 90s, and others.</p>
<p>&nbsp;</p><br><br><br><br><br><br></body></html>