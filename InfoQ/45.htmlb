<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>林仕鼎谈数据中心计算（三）：学术界与工业界，以及未来展望</h3><p>在<a href="http://www.infoq.com/cn/articles/linshiding-on-sddc-design-01;jsessionid=1898572497D8A5A310F1C8BFD61832D7">《林仕鼎谈数据中心计算》的第一部分</a>，百度基础体系首席架构师林仕鼎先生提到要将数据中心当做逻辑上的一台机器来设计架构。在<a href="http://www.infoq.com/cn/articles/linshiding-on-sddc-design-02;jsessionid=1898572497D8A5A310F1C8BFD61832D7">之后的对话</a>中，林仕鼎具体介绍了对数据中心的存储资源做逻辑分层、物理映射的思路。</p><p>在接下来的对话中，林仕鼎会介绍他对于“为什么以前的系统做的并不是很好”的理解，以及“为什么会想要去做这个系统”的原因。此外，对话中也会涉及到他对12306系统的部分解读，以及一些相关问题的看法。</p><p><strong>InfoQ：像您提到的将数据中心看做一台PC、逻辑分层这样的实现思路，为什么很少看到其他人来这样做呢？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>我做这个架构设计，不是单纯为了用，而是会去考虑这个东西本质上应该是什么样的，然后再去做，而且带着业界去做。我们讲创新，创新也是有要求的，你得知道这个东西能这么做，同时你要有能力去做这个东西。</p> 
 <p>为什么会想到这么去做这个系统？我个人是这样一个背景：我既写程序去做这个系统，同时我也做系统研究。大部分互联网公司里面，主要是工程师去做这个东西，他们很多只是去了解开源那些项目，但其实并不知道该怎么去做。很多开源项目，说实话，设计水平并不是很高。但是纯学术界的东西也不行：他们有设计思路，但是从设计思路到代码，还有很多问题。比如刚才我说的，我们做个系统，逻辑上有很多层，但具体对应到数据中心的时候，你怎么去做这个映射，这里有很多东西需要考虑。就算你想的很好，这个实现难度也是很高的，你怎么去把并行度发挥出来，让系统去异步的运行，这其中涉及到很多代码的复杂度。</p> 
 <p>首先，以前有那么多人在这方面想过什么，有什么进展，这个你要知道。整个领域有很多流派，你要有一个整体的了解。而对于这些，很多工程师实际上是不知道的。就好像我一开始看到Linux，我会觉得不就是应该这么做么，除了这样做还能怎么做呢？当时的我根本不知道有什么更好的做法，因为我没有跳出来看。你要看到抽象的东西，同时又要写实际的代码，一次一次的迭代，把这个东西实现。</p> 
 <p>研究系统跟生产系统有两点根本性的不同。一个是规模：研究系统一般跑的也就是几十台机器，这已经很大了。而几十台机器跑起来跟几千台机器跑起来，完全是不一样的。把几十台机器当做一台机器来看，和把几千台机器当做一台机器来看，完全是不一样的。如果没有做过，根本体会不到其中的区别。比如刚才说的层次结构，在几十台机器的时候，根本就不明显。第二个不同在于，研究系统面临的压力是不一样的，用户请求的类型是不一样的。刚来百度的时候，我做的第一个存储系统，原型只用了半年就做出来了，但是把它变成生产系统，又花了一年的时间。也就是说，三分之一的时间就完成了从设计到实现的过程，三分之二的时间用于把它变成真正可用的系统。</p> 
 <p>做一个存储系统包含三个层面：第一是存储系统本身对数据的组织，第二它是个分布式系统，第三它是个操作系统。数据的组织就是说你怎么去做文件系统，怎么去做对象，怎么去做数据库，这些涉及到数据该怎么存，索引该怎么建，修改的逻辑应该是什么样子——比如是原地修改，还是在后面索引操作等等。分布式系统要解决的就是数据怎么去分布的问题，解决冗余的问题。操作系统实际上就是如何在面临压力的时候合理的调度资源，如何去实现硬件所能提供的最大资源输出，同时保持稳定。现在很多人去做数据库，其实那只是在第一个层面，其他两个层面还是在用以前的东西。真正做一个完整的存储系统是非常难的。</p> 
</blockquote><p><strong>InfoQ：从原型到真正可用的系统，最大的难点在哪里？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>其实我之前也说过一些，比如12306。一个系统最复杂的地方不在于性能有多高，而是在于大量压力下的稳定性。比如说，我可能同时有人有scan的需求，有人有写的需求，有人有读的请求。Scan，写，读，需要消耗的资源是完全不一样的。读写可以并行，随机读写和Scan可以并行，Scan和Scan也可以并行。这时候你会面临很多问题：当几十个大的Scan请求进来的时候，我不能让资源被它消耗光，我要留一些出来去响应写和读。</p> 
 <p>即使同样是Scan，也有其他的问题要考虑：假如有80%的Scan的接收方很慢，这会拖累别的也很慢。这是很正常的情况。很多的系统会这样设计，比如我只支持10个Scan。因为Scan需要的资源实在太高，其他的请求只能等着。这是不合理的。最合理的情况应该是这样：我应该可以支持无穷多个并发的请求——只要我的能力允许；然后，我会按照对方能够接受的最大的可能，再均匀的分我这边的资源。假如说我这边只能发每秒100MB的数据，有两个请求进来，请求A每秒只能接收1MB，B每秒能接收200MB。有时候可能会出现这种情况：我把100MB全给了B，A这边一点都拿不到；还有另一种情况就是，A这边1MB进来的时候，A收1MB，B也只能收1MB。也就是说，系统中最慢的出口把整个系统都拖慢了。这已经算是好的了，因为很多系统在这种情况下已经不工作了，刚才说的是两个的情况，实际上往往是一堆进来。</p> 
 <p>最好的情况是，两个人进来的时候，我系统这边先分成一半一半，提供出去。那么A这边其实资源有富余，他只能收1MB，我给他50MB，他是受不了的。这个时候我再把多出来的49MB分给B。也就是说，先平均的分配资源，然后对消耗不了的多余资源进行再次分配，你就能得到最好的体验。</p> 
 <p>刚才说的这个情况比较简单，量大了之后要实现这个规则就非常复杂了。而且，不同的请求消耗的资源是完全不一样的。好比Scan消耗的资源可能是一次随机查询的几万倍，持续的时间也更长。假如我这时候有100个随机请求进来，然后又有100个顺序的Scan进来，这时候我要怎样分配，让大家都觉得可以接受？这就是12306系统遇到的问题。有些查询的消耗是其他查询的几十倍、几百倍，而且总的进来的数量一定是超过极限的。这不是简单的排序可以解决的，因为这是一个动态的过程：你这一个请求进来要消耗多少资源，不是你当时能够决定的。</p> 
 <p>什么是高性能系统？能够实现硬件所能够提供的最大能力输出。这是动态的，而不是绝对的。很多人说，一秒100个查询算不算高？1万个查询算不算高？十万个查询算不算高？这都不一定。</p> 
</blockquote><p><strong>InfoQ：那么，即使百度、淘宝来做12306，也会遇到这个问题么？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>12306遇到的问题，说实话，现在业界能够解决好的并不多。淘宝的交易系统压力其实并不高，压力主要在搜索、读照片这块。12306有一个更大的问题，就是它的后台要跟原先的老系统集成，很可能是ERP类的系统，那个系统的部件是很慢的。我们的系统里面一定是很多部件组成的，每一个部件都是输入输出。有些部件可能每秒只能支持100个并发，有些部件每秒可能只能支持1个并发，你给他10个并发，它就死掉了。排队当然一定是要有的，我通过上游来控制他的压力。但是你怎样做一个合理的排队？由于每个请求消耗的资源不一样，如果你只是做一个静态的限定，让固定数量的请求进来，那要么会因为口子设定小了而浪费，要么会因为口子设定大了而仍然导致部件死掉。</p> 
</blockquote><p><strong>InfoQ：不过话说回来，2013年的12306事实上可用性已经比之前一年提高很多了，虽然买到票仍然很难，但至少不会总出现页面刷不出来的问题。</strong></p><p>林仕鼎：</p><blockquote> 
 <p>据我猜测，今年他们做了一个事情，把transaction和query分开了。transaction的压力其实很小，毕竟票量也就那么多，只要一分开，query多用一些cache，一点问题都没有。</p> 
</blockquote><p>（InfoQ编辑注：林仕鼎在自己的博客上写过三篇文章，专门评论火车票系统架构的设计。具体请见：<a href="http://qing.blog.sina.com.cn/2244218960/85c41050330009xm.html">简单讨论火车票系统后面的架构设计</a>，<a href="http://qing.blog.sina.com.cn/2244218960/85c4105033000a3v.html">​再谈谈火车票系统</a>，<a href="http://qing.blog.sina.com.cn/2244218960/85c4105033000ab2.html">三谈火车票系统</a>）</p><p><strong>InfoQ：百度用ARM做存储服务器，您对冷数据和热数据的存储有什么建议？比如说网盘（共享居多）服务，需要定制什么样的存储服务器好？在线备份（写多读少）服务怎样定制比较好？又比如对IOPS要求高的数据库存储服务怎样定制比较好？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>我们希望硬件比较统一，而不希望有很多种的硬件。需求的情况不同，我们尽可能的通过软件来处理这种不同。从我的立场，我做了CCDB之后，就希望只有一套存储架构，用一套架构适配到不同的需求。之前提到的逻辑上的三个层，我映射到硬件上去，假如说你的场景是写多读少，那我Flash这层就不给你了，直接把硬盘给你。具体实现是在部署的时候，你会有个配置，配置上你定义它没有Flash，只有硬盘，硬盘无非两种，SATA或者SAS，现在有了Flash之后，其实我们尽量都不用SAS了，只用SATA+Flash。所以，我们把定制的这个问题变成了配置问题，硬件上都采用一个模型架构，全部标准化。这是我们现在正在进行的工作。</p> 
</blockquote><p>（InfoQ编辑注：目前阿里、腾讯、新浪等，也都在进行硬件机型标准化的工作，需求类似的服务器硬件机型很多都在进行合并。虽然可能会造成闲置资源的浪费，但由于批量采购统一的机型更加便宜，整体成本还是有所降低的。）</p><p><strong>InfoQ：百度目前在SDN这方面采用的什么方案，定制的交换机么？有哪些进一步的打算？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>SDN这块我们现在还没有特别的去做。对software define datacenter而言，网络部分我们一样希望它越简单越好。我们现在在内部尽量不做路由，只有交换，包括跨数据中心也都是交换。交换做起来就更简单了：你什么都不用做，只要提供两个点之间的通路就好了。</p> 
 <p>这其中涉及到收敛比。我们有大的交换机是中央交换机，一般是两个；然后机架上有机架交换机，一般是四台，都连到大交换机上去，对这个连接，你需要定义你的收敛比是多少。大交换机支持的规模大概在几千到几万左右。</p> 
 <p>之前也有人问我，我们数据中心有多大；那我们整个系统设计是紧耦合的，一个逻辑数据中心尽量限制在万台以内。如果超过了，我们再去分。所以对我们来说，不太需要去做SDN，简单的交换已经能够满足我们的需求。SDN我觉得主要还是针对复杂的路由，不适合用在数据中心里面，而更适合广域网。当然我们也在看。</p> 
</blockquote><p><strong>InfoQ：之前听说百度的负载均衡系统也是自主研发的，基于64核处理器，可为业务提供最大320G的负载均衡。另外之前<a href="http://www.infoq.com/cn/articles/vpanel-paas-routing-issue;jsessionid=1898572497D8A5A310F1C8BFD61832D7">听杜熙介绍过</a>，BAE这块的路由算法也是带智能的随机路由，会跳过繁忙或故障的后端节点。能简单介绍一下百度负载均衡系统的设计思路吗？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>负载均衡系统是我们自主研发的。BAE的路由算法我觉得不能算是路由算法，因为它并不是一个路由的概念，而是一个怎么处理故障的问题。一个请求来了，我怎么去分，这个负载均衡其实很简单：把请求发到某几台机子上面去，再在后端去进行选择。你只要知道目标机器的状态，这个实现并不难。</p> 
</blockquote><p><strong>InfoQ：通过软件定义硬件后，将会出现很多一体机，是否会出现各家做各家的，结果出现标准不统一，对软硬件发展不利呢？或者会出现另外新标准？百度是否会出新标准呢，像以前提的ARM存储服务器一样。</strong></p><p>林仕鼎：</p><blockquote> 
 <p>我倒不觉得会出现很多一体机，因为一体机更多是终端层面的。软件定义数据中心之后，整个数据中心成为一台机器，里面的都是部件，软件肯定是单独做，硬件厂商做的只是部件，不会成为一体机的。我们对硬件的要求很简单，硬件能把硬件的工作做好就行了，最好是简单、傻（dummy），同时对外暴露所有可编程的接口（programmable），这就行了。</p> 
 <p>新标准我们肯定是在做的，包括ARM服务器，机架——我们说可编程，不仅是服务器可编程，包括服务器周边的东西也都需要可编程，这包括机架，比如温度之类的传感器，我们都要可编程。</p> 
</blockquote><p><strong>InfoQ：去年，Frank Frankovsky在Open Compute的网站上提到他们的Open Rack和百度、腾讯设计的Project Scorpio这两个规范计划在2013年合并。这个合并的计划进展是否顺利？</strong></p><p>林仕鼎：</p><blockquote> 
 <p>我这边没有听说过合并的计划。其实在我看来，Open Rack其实做的并不是特别好，还不如我们自己做的。未来我们也希望能够越来越多的将我们的这些思路和实现分享出去，也多开源一些东西出来，让大家了解我们在做些什么，也带着业界一起前进。</p> 
</blockquote><p>在4月25日<a href="http://www.qconbeijing.com/">QCon北京</a>的主题演讲上，林仕鼎将会带来主题为《<a href="http://www.qconbeijing.com/speaker.php?id=213">架构设计与架构师</a>》的分享，就系统领域中基本的存储、计算、分布式技术以及服务模型进行分析，从另一种视角看待这些领域问题。敬请期待！</p><div class="clearer-space"></div><br><br><br><br><br><br></body></html>