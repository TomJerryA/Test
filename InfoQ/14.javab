<html><head><meta http-equiv="content-type" content="text/html; charset=utf-8" /></head><body><h3>How LinkedIn Uses Apache Samza</h3><p><i>Apache Samza is a stream processor LinkedIn recently open-sourced. In his presentation, Samza: Real-time Stream Processing at LinkedIn, Chris Riccomini <a href="http://www.infoq.com/presentations/samza-linkedin">discusses</a> Samza's feature set, how Samza integrates with YARN and Kafka, how it's used at LinkedIn, and what's next on the roadmap.</i></p>
<p><b><small>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Presentation transcript edited by <a href="http://www.infoq.com/author/Roopesh-Shenoy">Roopesh Shenoy</a></small></b></p>
<p>Apache Samza is a stream processor LinkedIn recently open-sourced. In his presentation, <a href="http://www.infoq.com/presentations/samza-linkedin">Samza: Real-time Stream Processing at LinkedIn</a>, Chris Riccomini discusses <a href="http://samza.incubator.apache.org/">Samza</a>'s feature set, how Samza integrates with YARN and Kafka, how it's used at LinkedIn, and what's next on the roadmap.</p>
<p>Bulk of processing that happens at LinkedIn is RPC-style data processing, where one expects a very fast response. On the other end of their response latency spectrum, they have batch processing, for which they use Hadoop for quite a bit. Hadoop processing and batch processing typically happens after the fact, often hours later.</p>
<p>There's this gap between synchronous RPC processing, where the user is actively waiting for a response, and this Hadoop-style processing which despite efforts to shrink it still it takes a long time to run through.<i> </i></p>
<p><img src="http://www.infoq.com/resource/articles/linkedin-samza/en/resources/3Fig1.png" alt="" _href="img://3Fig1.png" _p="true" /></p>
<div id="lowerFullwidthVCR"></div>
<p>That's where Samza fits in. This is where we can process stuff asynchronously, but we're also not waiting for hours. It typically operates in the order of milliseconds to minutes. The idea is to process stuff relatively quickly and get the data back to wherever it needs to be, whether that's a downstream system or some real-time service.</p>
<p>Chris mentions that right now, this stream processing is the worst-supported in terms of tooling and environment.</p>
<p>LinkedIn sees a lot of use cases for this type of processing –</p>
<ul> 
 <li>Newsfeed displays when people move to another company, when they like an article, when they join a group, et cetera.<i> </i></li> 
</ul>
<p>News is latency-sensitive and if you use Hadoop to batch-compute it, you might be getting responses hours or maybe even a day later. It is important to get trending articles in News pretty quickly.</p>
<ul> 
 <li>Advertising – getting relevant advertisements, as well as tracking and monitoring ad display, clicks and other metrics</li> 
 <li>Sophisticated monitoring that allows performing of complex querys like &quot;the top five slowest pages for the last minute.&quot;</li> 
</ul>
<h2>Existing Ecosystem at LinkedIn</h2>
<p>The existing ecosystem at LinkedIn has had a huge influence in the motivation behind Samza as well as it’s architecture. Hence it is important to have at least a glimpse of what this looks like before diving into Samza.</p>
<p><a href="https://kafka.apache.org/">Kafka</a> is an open-source project that LinkedIn released a few years ago. It is a messaging system that fulfills two needs – message-queuing and log aggregation. All of LinkedIn’s user activity, all the metrics and monitoring data, and even database changes go into this.</p>
<p>LinkedIn also has a specialized system called <a href="http://data.linkedin.com/projects/databus">Databus</a>, which models all of their databases as a stream. It is like a database with the latest data for each key-value pair. But as this database mutates, you can actually model that set of mutations as a stream. Each individual change is a message in that stream.</p>
<p>Because LinkedIn has Kafka and because they’ve integrated with it for the past few years, a lot of data at LinkedIn, almost all of it, is available in a stream format as opposed to a data format or on Hadoop.</p>
<h2>Motivation for Building Samza</h2>
<p>Chris mentions that when they began doing stream processing, with Kafka and all this data in their system, they started with something like a web service that would start up, read messages from Kafka and do some processing, and then write the messages back out.</p>
<p>As they did this, they realized that there were a lot of problems that needed to be solved in order to make it really useful and scalable. Things like partitioning: how do you partition your stream? How do you partition your processor? How do you manage state, where state is defined essentially as something that you maintain in your processor between messages, or things like count if you're incrementing a counter every time a message comes in. How do you re-process?</p>
<p>With failure semantics, you get at least once, at most once, exactly once messaging. There is also non-determinism. If your stream processor is interacting with another system, whether it's a database or it's depending on time or the ordering of messages, how you deal with stuff that actually determines the output that you will end up sending?</p>
<p>Samza tries to address some of these problems.</p>
<h2>Samza Architecture</h2>
<p>The most basic element of Samza is a stream. The stream definition for Samza is much more rigid and heavyweight than you would expect from other stream processing systems. Other processing systems, such as Storm, tend to have very lightweight stream definitions to reduce latency, everything from, say, UDP to a straight-up TCP connection.</p>
<p>Samza goes the other direction. It wants its streams to be, for starters, partitions. It wants them to be ordered. If you read Message 3 and then Message 4, you are never going to get those inverted within a single partition. It also wants them to replayable, which means you should be able to go back to reread a message at a later date. It wants them to be fault-tolerant. If a host from Partition 1 disappears, it should still be readable on some other hosts. Also, the streams are usually infinite. Once you get to the end – say, Message 6 of Partition 0 – you would just try to reread the next message when it's available. It's not the case that you're finished.</p>
<p>This definition maps very well to Kafka, which LinkedIn uses as the streaming infrastructure for Samza.</p>
<p>There are many <a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/introduction/concepts.html">concepts</a> to understand within Samza. In a gist, they are –</p>
<ul> 
 <li><a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/container/streams.html">Streams</a> – Samza processes streams. A stream is composed of immutable messages of a similar type or category. The actual implementation can be provided via a messaging system such as Kafka (where each topic becomes a Samza Stream) or a database (table) or even Hadoop (a directory of files in HDFS)</li> 
</ul>
<p>Things like message ordering, batching are handled via streams.</p>
<ul> 
 <li>Jobs – a Samza job is code that performs logical transformation on a set of input streams to append messages to a set of output streams</li> 
 <li>Partitions – For scalability, each stream is broken into one or more partitions. Each partition is a totally ordered sequence of messages</li> 
 <li>Tasks – again for scalability, a job is distributed by breaking it into multiple tasks. The task consumes data from one partition for each of the job’s input streams</li> 
 <li>Containers – whereas partitions and tasks are logical units of parallelism, containers are unit physical parallelism. Each container is a unix process (or linux cgroup) and runs one or more tasks.</li> 
 <li><a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/container/task-runner.html">TaskRunner</a> – Taskrunner is Samza’s stream processing container. It manages startup, execution and shutdown of one or more StreamTask instances.</li> 
 <li><a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/container/checkpointing.html">Checkpointing</a> – Checkpointing is generally done to enable failure recovery. If a taskrunner goes down for some reason (hardware failure, for e.g.), when it comes back up, it should start consuming messages where it left off last – this is achieved via Checkpointing.</li> 
 <li><a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/container/state-management.html">State management</a> – Data that needs to be passed between processing of different messages can be called state – this can be something as simple as a keeping count or something a lot more complex. Samza allows tasks to maintain persistent, mutable, queryable state that is physically co-located with each task. The state is highly available: in the event of a task failure it will be restored when the task fails over to another machine.</li> 
</ul>
<p>This datastore is pluggable, but Samza comes with a key-value store out-of-the-box.</p>
<ul> 
 <li>YARN (Yet Another Resource Manager) is Hadoop v2’s biggest improvement over v1 – it separates the Map-Reduce Job tracker from the resource management and enables Map-reduce alternatives to use the same resource manager. Samza utilizes YARN to do cluster management, tracking failures, etc.</li> 
</ul>
<p>Samza provides a YARN ApplicationMaster and a YARN job runner out of the box.</p>
<p><img src="http://www.infoq.com/resource/articles/linkedin-samza/en/resources/1Fig2.png" alt="" _href="img://1Fig2.png" _p="true" /></p>
<p>You can understand how the various components (YARN, Kafka and Samza API) interact by looking at the detailed <a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/introduction/architecture.html">architecture</a>. Also read the overall <a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/">documentation</a> to understand each component in detail.</p>
<h2>Possible Improvements</h2>
<p>One of the advantages of using something like YARN with Samza is that it enables you to potentially run Samza on the same grid that you already run your draft tasks, test tasks, and MapReduce tasks. You could use the same infrastructure for all of that. However, LinkedIn currently does not run Samza in a multi-framework environment because the existing setup itself is quite experimental.</p>
<p>In order to get into a more multi-framework environment, Chris says that the process isolation would have to get a little better.</p>
<h2>Conclusion</h2>
<p>Samza is a relatively young project incubating at Apache so there's a lot of room to get involved. A good way to get started is with the <a href="http://samza.incubator.apache.org/startup/hello-samza/0.7.0/">hello-samza</a> project, which is a little thing that will get you up and running in about five minutes. It will let you play with a real-time change log from the Wikipedia servers to let you figure out what's going on in and give you a stream of stuff to play with.</p>
<p>The other stream processing project built on top of Hadoop is STORM. You can see a <a href="http://samza.incubator.apache.org/learn/documentation/0.7.0/comparisons/storm.html">comparison between Samza and STORM</a></p>
<h2>About the Author</h2>
<p><strong><img src="http://www.infoq.com/resource/articles/linkedin-samza/en/resources/Chris-Riccomini.jpg" vspace="3" hspace="3" align="left" alt="" _href="img://Chris-Riccomini.jpg" _p="true" />Chris Riccomini</strong> is a Staff Software Engineer at LinkedIn, where he's is currently working as a committer and PMC member for Apache Samza. He's been involved in a wide range of projects at LinkedIn, including, &quot;People You May Know&quot;, REST.li, Hadoop, engineering tooling, and OLAP systems. Prior to LinkedIn, he worked on data visualization and fraud modeling at PayPal.</p><br><br><br><br><br><br></body></html>